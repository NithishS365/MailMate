#!/usr/bin/env python3
"""
Quick Email Scraper Script for MailMate

Simple script to scrape Gmail emails and store them for use in MailMate.
This script provides an easy way to get started with real email data.
"""

import os
import sys
from pathlib import Path

# Add backend to path
backend_dir = Path(__file__).parent / 'backend'
sys.path.insert(0, str(backend_dir))

def main():
    """Main function to run email scraping."""
    from backend.email_scraper import EmailScraper
    
    print("ğŸ¯ MailMate Gmail Scraper")
    print("=" * 50)
    
    # Get email address
    email = input("ğŸ“§ Enter your Gmail address: ").strip()
    if not email:
        print("âŒ Email address is required")
        return
    
    # Ask for scraping options
    print("\nğŸ”§ Scraping Options:")
    print("1. Quick sync (INBOX only, last 100 emails)")
    print("2. Standard sync (INBOX + Sent, last 500 emails)")  
    print("3. Full sync (all folders, all emails)")
    print("4. Custom sync")
    
    choice = input("\nSelect option (1-4) [1]: ").strip() or "1"
    
    # Set up scraping parameters
    if choice == "1":
        folders = ["INBOX"]
        max_emails = 100
        print("ğŸ“‹ Quick sync: INBOX, 100 emails")
    elif choice == "2":
        folders = ["INBOX", "[Gmail]/Sent Mail"]
        max_emails = 500
        print("ğŸ“‹ Standard sync: INBOX + Sent, 500 emails each")
    elif choice == "3":
        folders = None
        max_emails = None
        print("ğŸ“‹ Full sync: All folders, all emails")
    else:
        # Custom options
        folder_input = input("ğŸ“ Folders (comma-separated) [INBOX]: ").strip()
        folders = [f.strip() for f in folder_input.split(",")] if folder_input else ["INBOX"]
        
        max_input = input("ğŸ“Š Max emails per folder (empty for all): ").strip()
        max_emails = int(max_input) if max_input.isdigit() else None
        
        print(f"ğŸ“‹ Custom sync: {folders}, {max_emails or 'all'} emails each")
    
    # Data directory
    data_dir = Path("data")
    data_dir.mkdir(exist_ok=True)
    
    try:
        print(f"\nğŸš€ Starting email scraping...")
        print(f"ğŸ“‚ Data will be saved to: {data_dir}")
        
        with EmailScraper(data_dir=str(data_dir)) as scraper:
            # Connect to Gmail
            print("\nğŸ” Connecting to Gmail...")
            print("   (This will open a browser window for OAuth authentication)")
            scraper.connect_gmail(email)
            print("âœ… Connected successfully!")
            
            # Show available folders if doing full sync
            if folders is None:
                available_folders = scraper.get_folder_list()
                print(f"\nğŸ“‹ Available folders ({len(available_folders)}):")
                for folder in available_folders[:10]:  # Show first 10
                    print(f"   â€¢ {folder}")
                if len(available_folders) > 10:
                    print(f"   â€¢ ... and {len(available_folders) - 10} more")
            
            # Scrape emails
            print(f"\nğŸ“¥ Scraping emails...")
            emails = scraper.scrape_all_folders(
                folders=folders,
                max_emails_per_folder=max_emails,
                incremental=True
            )
            
            if not emails:
                print("âš ï¸  No emails found to scrape")
                return
            
            # Save emails
            print(f"\nğŸ’¾ Saving {len(emails)} emails...")
            file_path = scraper.save_emails(emails, format='json')
            
            # Show results
            stats = scraper.get_stats()
            print("\n" + "=" * 50)
            print("âœ… Scraping completed successfully!")
            print(f"ğŸ“Š Results:")
            print(f"   â€¢ Emails scraped: {stats['total_new']}")
            print(f"   â€¢ Folders processed: {stats['total_folders']}")
            print(f"   â€¢ Output file: {file_path}")
            
            if stats.get('duration_seconds'):
                print(f"   â€¢ Time taken: {stats['duration_seconds']:.1f} seconds")
                print(f"   â€¢ Speed: {stats.get('emails_per_second', 0):.1f} emails/sec")
            
            print(f"\nğŸ‰ Your emails are now saved and ready to use!")
            print(f"ğŸ“ File location: {file_path}")
            print(f"\nğŸ”„ To use this data in MailMate:")
            print(f"   1. Start the MailMate server: python mailmate_server.py")
            print(f"   2. The server will automatically load your scraped emails")
            print(f"   3. Visit http://localhost:5000 to view your dashboard")
            
            if stats['errors']:
                print(f"\nâš ï¸  Encountered {len(stats['errors'])} errors during scraping")
                print("   Check the logs for details")
                
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  Scraping cancelled by user")
    except Exception as e:
        print(f"\nâŒ Error during scraping: {e}")
        print("\nğŸ”§ Troubleshooting:")
        print("   â€¢ Make sure you have a valid Gmail account")
        print("   â€¢ Check your internet connection")
        print("   â€¢ Ensure OAuth2 credentials are set up correctly")
        print("   â€¢ Run with --verbose for more detailed error information")


if __name__ == "__main__":
    main()